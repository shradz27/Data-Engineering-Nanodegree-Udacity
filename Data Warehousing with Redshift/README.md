# **Udacity Data Engineer Project - Data Warehouse with AWS Redshift**

Sparkify, has grown their user base and song database and wants to move their processes and data onto the cloud. Their data resides in S3, in a directory of JSON logs on user activity on the app, as well as a directory with JSON metadata on the songs in their app. 
The ETL process extracts the data from S3, stages it in Redshift, then transforms the data into a set of dimensional tables in a star schema. To improve processing speeds for query analysis, a specified distribution strategy is used for partitioning the tables across Redshift nodes using dist keys and ordering the data using sort keys.

## Project Datasets

There are two datasets that reside in AWS S3 buckets to be combined for our data warehouse. Each are stored as `*.json` files in a folder hierachy related to either songs or dates.

### Song Dataset

The first dataset is a subset of real data from the Million Song Dataset. Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID. For example, here are filepaths to two files in this dataset.

```
song_data/A/B/C/TRABCEI128F424C983.json
song_data/A/A/B/TRAABJL12903CDCF1A.json
```

### Log Dataset
The second dataset consists of log files in JSON format generated by this event simulator based on the songs in the dataset above. These simulate app activity logs from an imaginary music streaming app based on configuration settings.

The log files in the dataset you'll be working with are partitioned by year and month. For example, here are filepaths to two files in this dataset.
```
log_data/2018/11/2018-11-12-events.json
log_data/2018/11/2018-11-13-events.json
```

## Project Components

- `create_table.py` : Python script to perform SQL-Statements for (re-)creating database and tables

- `sql_queries.py` : Python script containing SQL-Statements used by create_tables.py and etl.py

- `etl.py` : Python script to extract the needed information from Song and Log data inside the S3 buckets and parsing/inserting them to the created Redshift database schema and tables


## Use

To create the database tables and run the ETL pipeline, run the following two files in the order that they are listed below

To create tables:
```bash
python create_tables.py
```
To fill tables via ETL:
```bash
python etl.py
```